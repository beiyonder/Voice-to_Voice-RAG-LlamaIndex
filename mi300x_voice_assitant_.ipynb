{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e693f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update package list and install curl (if not present)\n",
    "sudo apt-get update && sudo apt-get install -y curl\n",
    "\n",
    "# Install Ollama\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Launch the Ollama server in the background\n",
    "ollama serve &\n",
    "\n",
    "# Pull the Llama 3 model (this may take a few minutes)\n",
    "ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required Python packages in one go\n",
    "pip install llama-index llama-index-llms-ollama llama-index-embeddings-ollama llama-index-embeddings-huggingface openai-whisper transformers ChatTTS jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the JupyterLab server, making it accessible from your local machine\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Imports for Speech to Text\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "# Imports for RAG Model\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Imports for Text to Speech\n",
    "import ChatTTS\n",
    "import torchaudio\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f077ecf",
   "metadata": {},
   "source": [
    "## Setting up the env\n",
    "\n",
    "# Set the environment variable for experimental features (optional)\n",
    "os.environ['TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL'] = '1'\n",
    "os.environ['HIP_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check GPU availability and properties\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU (no GPU detected)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b12ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transcribe speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L https://raw.githubusercontent.com/ROCm/gpuaidev/main/docs/notebooks/assets/summarize_question.wav -o summarize_question.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e081f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FILE = \"summarize_question.wav\"\n",
    "Audio(AUDIO_FILE, rate=24_000, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c0ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech-to-Text with Whisper\n",
    "try:\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(AUDIO_FILE)\n",
    "    input_text = result[\"text\"]\n",
    "    print(f\"Transcribed text: {input_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in speech-to-text: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866c72d",
   "metadata": {},
   "source": [
    "## Integrating the RAG Model  \n",
    "To utilize a Retrieval-Augmented Generation (RAG) model, supply the context you want the language model to reference when answering queries. In this example, documents from the `data` folder are used as context. If you don’t have any documents yet, you can add your own or download the sample provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f385b64",
   "metadata": {},
   "source": [
    "DATA_DIR = \"./data\"\n",
    "\n",
    "# Check if the data directory exists, and create it if it doesn't\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    print(f\"Data directory '{DATA_DIR}' created. Please add a file of your choosing or use the cell below to download sample text.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    # Check if data directory is empty\n",
    "    if not os.listdir(DATA_DIR):\n",
    "        print(f\"Data directory '{DATA_DIR}' is empty. Please add a file of your choosing or use the cell below to download sample text.\")\n",
    "        exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the data dictionary is empty then run this:\n",
    "\n",
    "!mkdir -p data && curl -L https://www.gutenberg.org/cache/epub/11/pg11.txt -o data/pg11.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the files in your data directory\n",
    "print(\"Files in data directory:\", os.listdir(\"data\"))\n",
    "documents = SimpleDirectoryReader(DATA_DIR).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2706a7",
   "metadata": {},
   "source": [
    "For the embedding model, use “bge-base” from HuggingFaceEmbedding. Confirm that the Ollama server is running because it supplies Llama-3 for the LLM.\n",
    "\n",
    "Next, create a VectorStoreIndex from the loaded documents and initialize a query engine with the index. Then issue your query using the text output from the Whisper model. Print the response so you can compare it against the audio output in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3357f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding and LLM models\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "try:\n",
    "    Settings.llm = Ollama(model=\"llama3\", request_timeout=360.0)\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama server: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Build and query the vector index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(streaming=True, response_mode=\"compact\", similarity_top_k=3)\n",
    "response = query_engine.query(input_text)\n",
    "\n",
    "# Function to convert StreamingResponse to string\n",
    "def streaming_response_to_string(streaming_response):\n",
    "    text = \"\"\n",
    "    for chunk in streaming_response.response_gen:\n",
    "        if isinstance(chunk, dict) and \"text\" in chunk:\n",
    "            text += chunk[\"text\"]\n",
    "        else:\n",
    "            text += str(chunk)\n",
    "    return text\n",
    "\n",
    "# Convert response to string\n",
    "response_text = streaming_response_to_string(response)\n",
    "print(f\"Generated response: {response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842e64b",
   "metadata": {},
   "source": [
    "## Perform text-to-speech conversion\n",
    "The following example performs text-to-speech conversion using the ChatTTS library and saves the output audio to a file.\n",
    "\n",
    "This example uses athe following constants:\n",
    "\n",
    "OUTPUT_AUDIO_FILE (str): The name of the output audio file.\n",
    "\n",
    "SAMPLE_RATE (int): The sample rate for the output audio file.\n",
    "\n",
    "It provides the following functionality:\n",
    "\n",
    "Initializes a ChatTTS.Chat object.\n",
    "\n",
    "Loads the chat model without compilation for faster loading. (Set compile=True for better performance.)\n",
    "\n",
    "Converts the response text from the previous step to speech.\n",
    "\n",
    "Saves the generated audio to the specified output file using torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176032d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_AUDIO_FILE = \"voice_pipeline_response.wav\"\n",
    "SAMPLE_RATE = 24000\n",
    "\n",
    "# Text cleanup function for TTS\n",
    "def sanitize_input(text):\n",
    "    sanitized_text = text.replace('-', '')  # Remove hyphens\n",
    "    sanitized_text = sanitized_text.replace('(', '').replace(')', '')  # Remove parentheses\n",
    "    return sanitized_text.strip()\n",
    "\n",
    "# Text-to-Speech processing\n",
    "try:\n",
    "    sanitized_response = re.sub(r\"[^a-zA-Z0-9.,?! ]\", \"\", response_text)  # Remove special characters\n",
    "    print(f\"Sanitized response for TTS: {sanitized_response}\")\n",
    "    sanitized_response = [sanitized_response]\n",
    "\n",
    "    chat = ChatTTS.Chat()\n",
    "    chat.load(compile=False) # Set to True for better performance\n",
    "\n",
    "    params_infer_code = ChatTTS.Chat.InferCodeParams(\n",
    "        spk_emb = chat.sample_random_speaker(),\n",
    "    )\n",
    "\n",
    "    wavs = chat.infer(\n",
    "        sanitized_response,\n",
    "        params_infer_code=params_infer_code,\n",
    "    )\n",
    "    try:\n",
    "        torchaudio.save(OUTPUT_AUDIO_FILE, torch.from_numpy(wavs[0]).unsqueeze(0), SAMPLE_RATE)\n",
    "    except:\n",
    "        torchaudio.save(OUTPUT_AUDIO_FILE, torch.from_numpy(wavs[0]), SAMPLE_RATE)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in text-to-speech: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "finally:\n",
    "    if 'chat' in locals():\n",
    "        chat.unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play the following cell to hear the generated speech.\n",
    "\n",
    "Audio(wavs[0], rate=24_000, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
